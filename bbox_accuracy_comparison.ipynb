{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafe732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from model_evaluation import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c01cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\"apple\", \"avocado\", \"banana\", \"kiwi\", \"lemon\", \"orange\", \"pear\", \"pomegranate\", \"strawberry\", \"watermelon\"]\n",
    "TEST_IMAGES = \"dataset/split/test/images\"\n",
    "TEST_LABELS = \"dataset/split/test/labels\"\n",
    "CONFIDENCE = 0.5\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "RCNN_MODEL_WEIGHTS = Path(\"models/faster_rcnn_fruits.pth\")\n",
    "RCNN_OUTPUT_DIR = \"evaluation_results/faster_rcnn\"\n",
    "YOLOV8N_MODEL_WEIGHTS = Path(\"runs/fruits_yolov8n/weights/best.pt\")\n",
    "YOLOV8N_OUTPUT_DIR = \"evaluation_results/yolov8n\"\n",
    "YOLOV8M_MODEL_WEIGHTS = Path(\"runs/fruits_yolov8m/weights/best.pt\")\n",
    "YOLOV8M_OUTPUT_DIR = \"evaluation_results/yolov8m\"\n",
    "YOLO11L_MODEL_WEIGHTS = Path(\"runs/fruits_yolov11l/weights/best.pt\")\n",
    "YOLO11L_OUTPUT_DIR = \"evaluation_results/yolov11l\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dceb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d2773",
   "metadata": {},
   "source": [
    "Evaluation for Faster_RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a267f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Faster R-CNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel27/miniconda3/envs/mlenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/daniel27/miniconda3/envs/mlenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 127 test images\n",
      "Processing 10/127...\n",
      "Processing 20/127...\n",
      "Processing 30/127...\n",
      "Processing 40/127...\n",
      "Processing 50/127...\n",
      "Processing 60/127...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel27/miniconda3/envs/mlenv/lib/python3.11/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 70/127...\n",
      "Processing 80/127...\n",
      "Processing 90/127...\n",
      "Processing 100/127...\n",
      "Processing 110/127...\n",
      "Processing 120/127...\n",
      "\n",
      "Generating evaluation report...\n",
      "============================================================\n",
      "BOUNDING BOX EVALUATION REPORT: Faster R-CNN\n",
      "============================================================\n",
      "\n",
      "Confidence Threshold: 0.5\n",
      "\n",
      "------------------------------------------------------------\n",
      "PRIMARY METRICS\n",
      "------------------------------------------------------------\n",
      "mAP@0.5:          0.8032\n",
      "mAP@0.75:         0.7086\n",
      "mAP@[0.5:0.95]:   0.6415\n",
      "Mean IoU:         0.8877\n",
      "\n",
      "------------------------------------------------------------\n",
      "DETECTION METRICS (IoU@0.5)\n",
      "------------------------------------------------------------\n",
      "Recall:           0.8709\n",
      "Precision:        0.6921\n",
      "F1-Score:         0.7713\n",
      "\n",
      "------------------------------------------------------------\n",
      "ERROR ANALYSIS\n",
      "------------------------------------------------------------\n",
      "False Positives:  141\n",
      "False Negatives:  47\n",
      "Total Predictions: 458\n",
      "Total Ground Truths: 364\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "Report saved to: evaluation_results/faster_rcnn\n",
      "\n",
      "============================================================\n",
      "FASTER_RCNN MODEL EVALUATION COMPLETE!\n",
      "============================================================\n",
      "mAP@0.5: 0.8032\n",
      "mAP@0.75: 0.7086\n",
      "Recall: 0.8709\n",
      "Precision: 0.6921\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "rcnn_metrics = evaluate_faster_rcnn(\n",
    "    model_weights_path=RCNN_MODEL_WEIGHTS,\n",
    "    test_image_dir=TEST_IMAGES,\n",
    "    test_label_dir=TEST_LABELS,\n",
    "    output_dir=RCNN_OUTPUT_DIR,\n",
    "    confidence_threshold=CONFIDENCE,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "all_results['Faster_RCNN'] = rcnn_metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FASTER_RCNN MODEL EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"mAP@0.5: {rcnn_metrics.map_50:.4f}\")\n",
    "print(f\"mAP@0.75: {rcnn_metrics.map_75:.4f}\")\n",
    "print(f\"Recall: {rcnn_metrics.recall_50:.4f}\")\n",
    "print(f\"Precision: {rcnn_metrics.precision_50:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878d83a",
   "metadata": {},
   "source": [
    "Evalution for Yolov8n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf91a73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model from runs/fruits_yolov8n/weights/best.pt...\n",
      "Found 127 test images\n",
      "Processing 10/127...\n",
      "Processing 20/127...\n",
      "Processing 30/127...\n",
      "Processing 40/127...\n",
      "Processing 50/127...\n",
      "Processing 60/127...\n",
      "Processing 70/127...\n",
      "Processing 80/127...\n",
      "Processing 90/127...\n",
      "Processing 100/127...\n",
      "Processing 110/127...\n",
      "Processing 120/127...\n",
      "\n",
      "Generating evaluation report...\n",
      "============================================================\n",
      "BOUNDING BOX EVALUATION REPORT: YOLO (best)\n",
      "============================================================\n",
      "\n",
      "Confidence Threshold: 0.5\n",
      "\n",
      "------------------------------------------------------------\n",
      "PRIMARY METRICS\n",
      "------------------------------------------------------------\n",
      "mAP@0.5:          0.7970\n",
      "mAP@0.75:         0.7002\n",
      "mAP@[0.5:0.95]:   0.6372\n",
      "Mean IoU:         0.8908\n",
      "\n",
      "------------------------------------------------------------\n",
      "DETECTION METRICS (IoU@0.5)\n",
      "------------------------------------------------------------\n",
      "Recall:           0.8297\n",
      "Precision:        0.8629\n",
      "F1-Score:         0.8459\n",
      "\n",
      "------------------------------------------------------------\n",
      "ERROR ANALYSIS\n",
      "------------------------------------------------------------\n",
      "False Positives:  48\n",
      "False Negatives:  62\n",
      "Total Predictions: 350\n",
      "Total Ground Truths: 364\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "Report saved to: evaluation_results/yolov8n\n",
      "\n",
      "============================================================\n",
      "YOLOV8N MODEL EVALUATION COMPLETE!\n",
      "============================================================\n",
      "mAP@0.5: 0.7970\n",
      "mAP@0.75: 0.7002\n",
      "Recall: 0.8297\n",
      "Precision: 0.8629\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "yolov8n_metrics = evaluate_yolo(\n",
    "    model_weights_path=YOLOV8N_MODEL_WEIGHTS,\n",
    "    test_image_dir=TEST_IMAGES,\n",
    "    test_label_dir=TEST_LABELS,\n",
    "    output_dir=YOLOV8N_OUTPUT_DIR,\n",
    "    confidence_threshold=CONFIDENCE,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "all_results['YoloV8n'] = yolov8n_metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"YOLOV8N MODEL EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"mAP@0.5: {yolov8n_metrics.map_50:.4f}\")\n",
    "print(f\"mAP@0.75: {yolov8n_metrics.map_75:.4f}\")\n",
    "print(f\"Recall: {yolov8n_metrics.recall_50:.4f}\")\n",
    "print(f\"Precision: {yolov8n_metrics.precision_50:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1689e",
   "metadata": {},
   "source": [
    "Evalution for Yolov8m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46563deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model from runs/fruits_yolov8m/weights/best.pt...\n",
      "Found 127 test images\n",
      "Processing 10/127...\n",
      "Processing 20/127...\n",
      "Processing 30/127...\n",
      "Processing 40/127...\n",
      "Processing 50/127...\n",
      "Processing 60/127...\n",
      "Processing 70/127...\n",
      "Processing 80/127...\n",
      "Processing 90/127...\n",
      "Processing 100/127...\n",
      "Processing 110/127...\n",
      "Processing 120/127...\n",
      "\n",
      "Generating evaluation report...\n",
      "============================================================\n",
      "BOUNDING BOX EVALUATION REPORT: YOLO (best)\n",
      "============================================================\n",
      "\n",
      "Confidence Threshold: 0.5\n",
      "\n",
      "------------------------------------------------------------\n",
      "PRIMARY METRICS\n",
      "------------------------------------------------------------\n",
      "mAP@0.5:          0.8019\n",
      "mAP@0.75:         0.7036\n",
      "mAP@[0.5:0.95]:   0.6486\n",
      "Mean IoU:         0.8923\n",
      "\n",
      "------------------------------------------------------------\n",
      "DETECTION METRICS (IoU@0.5)\n",
      "------------------------------------------------------------\n",
      "Recall:           0.8407\n",
      "Precision:        0.8768\n",
      "F1-Score:         0.8583\n",
      "\n",
      "------------------------------------------------------------\n",
      "ERROR ANALYSIS\n",
      "------------------------------------------------------------\n",
      "False Positives:  43\n",
      "False Negatives:  58\n",
      "Total Predictions: 349\n",
      "Total Ground Truths: 364\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "Report saved to: evaluation_results/yolov8m\n",
      "\n",
      "============================================================\n",
      "YOLOV8M MODEL EVALUATION COMPLETE!\n",
      "============================================================\n",
      "mAP@0.5: 0.8019\n",
      "mAP@0.75: 0.7036\n",
      "Recall: 0.8407\n",
      "Precision: 0.8768\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "yolov8m_metrics = evaluate_yolo(\n",
    "    model_weights_path=YOLOV8M_MODEL_WEIGHTS,\n",
    "    test_image_dir=TEST_IMAGES,\n",
    "    test_label_dir=TEST_LABELS,\n",
    "    output_dir=YOLOV8M_OUTPUT_DIR,\n",
    "    confidence_threshold=CONFIDENCE,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "all_results['YoloV8m'] = yolov8m_metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"YOLOV8M MODEL EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"mAP@0.5: {yolov8m_metrics.map_50:.4f}\")\n",
    "print(f\"mAP@0.75: {yolov8m_metrics.map_75:.4f}\")\n",
    "print(f\"Recall: {yolov8m_metrics.recall_50:.4f}\")\n",
    "print(f\"Precision: {yolov8m_metrics.precision_50:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f84ec",
   "metadata": {},
   "source": [
    "Evalution for Yolo11l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d3d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model from runs/fruits_yolov11l/weights/best.pt...\n",
      "Found 127 test images\n",
      "Processing 10/127...\n",
      "Processing 20/127...\n",
      "Processing 30/127...\n",
      "Processing 40/127...\n",
      "Processing 50/127...\n",
      "Processing 60/127...\n",
      "Processing 70/127...\n",
      "Processing 80/127...\n",
      "Processing 90/127...\n",
      "Processing 100/127...\n",
      "Processing 110/127...\n",
      "Processing 120/127...\n",
      "\n",
      "Generating evaluation report...\n",
      "============================================================\n",
      "BOUNDING BOX EVALUATION REPORT: YOLO (best)\n",
      "============================================================\n",
      "\n",
      "Confidence Threshold: 0.5\n",
      "\n",
      "------------------------------------------------------------\n",
      "PRIMARY METRICS\n",
      "------------------------------------------------------------\n",
      "mAP@0.5:          0.7119\n",
      "mAP@0.75:         0.6930\n",
      "mAP@[0.5:0.95]:   0.5984\n",
      "Mean IoU:         0.8919\n",
      "\n",
      "------------------------------------------------------------\n",
      "DETECTION METRICS (IoU@0.5)\n",
      "------------------------------------------------------------\n",
      "Recall:           0.7802\n",
      "Precision:        0.8987\n",
      "F1-Score:         0.8353\n",
      "\n",
      "------------------------------------------------------------\n",
      "ERROR ANALYSIS\n",
      "------------------------------------------------------------\n",
      "False Positives:  32\n",
      "False Negatives:  80\n",
      "Total Predictions: 316\n",
      "Total Ground Truths: 364\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "Report saved to: evaluation_results/yolov11l\n",
      "\n",
      "============================================================\n",
      "YOLO11L MODEL EVALUATION COMPLETE!\n",
      "============================================================\n",
      "mAP@0.5: 0.7119\n",
      "mAP@0.75: 0.6930\n",
      "Recall: 0.7802\n",
      "Precision: 0.8987\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "yolo11l_metrics = evaluate_yolo(\n",
    "    model_weights_path=YOLO11L_MODEL_WEIGHTS,\n",
    "    test_image_dir=TEST_IMAGES,\n",
    "    test_label_dir=TEST_LABELS,\n",
    "    output_dir=YOLO11L_OUTPUT_DIR,\n",
    "    confidence_threshold=CONFIDENCE,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "all_results['Yolo11l'] = yolo11l_metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"YOLO11L MODEL EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"mAP@0.5: {yolo11l_metrics.map_50:.4f}\")\n",
    "print(f\"mAP@0.75: {yolo11l_metrics.map_75:.4f}\")\n",
    "print(f\"Recall: {yolo11l_metrics.recall_50:.4f}\")\n",
    "print(f\"Precision: {yolo11l_metrics.precision_50:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88701d8",
   "metadata": {},
   "source": [
    "Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d3c60fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "MODEL COMPARISON - BOUNDING BOX ACCURACY\n",
      "==========================================================================================\n",
      "Model        mAP@0.5    mAP@0.75   mAP@.5:.95   Mean IoU   Recall     Precision \n",
      "------------------------------------------------------------------------------------------\n",
      "Faster_RCNN  0.8032     0.7086     0.6415       0.8877     0.8709     0.6921    \n",
      "YoloV8n      0.7970     0.7002     0.6372       0.8908     0.8297     0.8629    \n",
      "YoloV8m      0.8019     0.7036     0.6486       0.8923     0.8407     0.8768    \n",
      "Yolo11l      0.7119     0.6930     0.5984       0.8919     0.7802     0.8987    \n",
      "==========================================================================================\n",
      "\n",
      "BEST MODELS:\n",
      "  Best mAP@0.5: Faster_RCNN\n",
      "  Best mAP@0.75: Faster_RCNN\n",
      "  Best Recall: Faster_RCNN\n",
      "  Best Precision: Yolo11l\n"
     ]
    }
   ],
   "source": [
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"MODEL COMPARISON - BOUNDING BOX ACCURACY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<12} {'mAP@0.5':<10} {'mAP@0.75':<10} {'mAP@.5:.95':<12} \"\n",
    "        f\"{'Mean IoU':<10} {'Recall':<10} {'Precision':<10}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for model_name, metrics in all_results.items():\n",
    "    f1 = (2 * metrics.precision_50 * metrics.recall_50 / \n",
    "            (metrics.precision_50 + metrics.recall_50) \n",
    "            if (metrics.precision_50 + metrics.recall_50) > 0 else 0)\n",
    "    \n",
    "    print(f\"{model_name:<12} {metrics.map_50:<10.4f} {metrics.map_75:<10.4f} \"\n",
    "            f\"{metrics.map_50_95:<12.4f} {metrics.mean_iou:<10.4f} \"\n",
    "            f\"{metrics.recall_50:<10.4f} {metrics.precision_50:<10.4f}\")\n",
    "\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nBEST MODELS:\")\n",
    "print(f\"  Best mAP@0.5: {max(all_results.items(), key=lambda x: x[1].map_50)[0]}\")\n",
    "print(f\"  Best mAP@0.75: {max(all_results.items(), key=lambda x: x[1].map_75)[0]}\")\n",
    "print(f\"  Best Recall: {max(all_results.items(), key=lambda x: x[1].recall_50)[0]}\")\n",
    "print(f\"  Best Precision: {max(all_results.items(), key=lambda x: x[1].precision_50)[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
